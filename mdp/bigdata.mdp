title: Framework per Big Data
subtitle: Hadoop e (Py)Spark
figure: images/db/overflow.png

---

title: Volume and velocity

- Big Data often characterized in terms of 5 Vs (+ others)
    - Volume, variety, velocity, value, veracity
- Challenges to traditional computational architectures
    - Handle large quantities of data (*volume*)
    - Readily react to their arrival (*velocity*)

---

title: A meta-definition

> Big data should be defined at any point in time as «data whose size forces us to look beyond the tried-and-true methods that are prevalent at that time.» *(Jacobs, 2009)*

- Meta-definition centered on *volume*
    - It ignores other Vs
- *Moving threshold*, for a dataset to be defined as “big data”
    - It grows together with technological advances

---

title: Big data pathologies

- Transaction processing and data storage are largely solved problems
- Data with a traditional RDBMS
    - Easier to *get in* than *get out*
    - Worst pathologies are related to **data analysis**

---

title: How much data?

- Nowadays, a dataset is “big”, if
    - It cannot be loaded and processed effectively with desktop tools for data analys and visualization
- Ex.: dataset of 7 billion records (Jacob, 2009)
- It can be saved in a common HDD
    - Simple questions (min, max, avg) can be answered quickly, by reading the file sequentially
    - More complex queries are difficult
- With some effort, it can be loaded in a RDMBS
    - Analysis through SQL queries is way too slow

---

title: Bounded vs Unbounded Dataset

- Bounded Dataset
    - A finite set of data, already available at the start
- Unbounded Dataset
    - Data arriving continuously
    - Often in a “streaming” scenario

---

title: Unbounded Dataset - Temporal domain

- Unbounded dataset characterized by two temporal domains
    - Event time: when the event occurs
    - Computation time: when data enter the processing system
- Often, non-constant difference between them: skew
- When the event time is important, skew can complicate the generation of correct results

---

title: Processing of bounded / unbounded dataset

- Bounded datasets
    - Often processed with batch systems; e.g. MapReduce
- Unbounded datasets
    - Processed in either batch or streaming mode
    - Data buffered in a windowing logic
    - Sort of bounded dataset, processed as batch
    - Incomplete data, if event time is important
    - A user session can be divided in multiple batches, if longer than a window

---

title: Free lunch is over

- For a long time, programs have taken advantage from growing hardware performances
- Limits to Moore's law
    - Physical limits make it hard to increase the transistor density
    - Heat dissipation make it hard to increase frequency
- *“The free lunch is over” (Sutter, 2004)*
    - Multi-core processors
    - Multi-threaded programs to exploit hardware parallelism
- Concurrent programming is hard
    - Deadlock, livelock, starvation, race, etc.
    - Some algorithms are intrinsically sequential

---

title: Amdahl's law

- A limit to program speedup (how much it is faster on a parallel hardware)
    - *T~~1~~ = T~~ser~~ + T~~par~~*
    - *f = T~~ser~~/T~~1~~*
    - *T~~P~~ ≥ T~~ser~~ + T~~par~~/P = fT~~1~~ + (1−f)T~~1~~/P*
    - *S~~P~~ = T~~1~~/T~~P~~ ≤ T~~1~~/(fT~~1~~ + (1−f)T~~1~~/P) = 1/(f + (1−f)/P)*
    - *S~~∞~~ ≤ lim~~P→∞~~ 1/(f + (1−f)/P) = 1/f*
- *f*: fraction of work which is sequential
- A program spending 1% of its execution time on a sequential task
    - Speedup limited to max 100 times faster, with unlimited parallelism

---

title: Distributed systems

- Limited resources on a single node -> more nodes
- Additional problems, wrt parallel systems
    - Is a node dead or very slow?
    - Is a task being executed?
    - How to reach consensus?
- Framework for distributed computing
    - Fault tolerance
    - Data distribution
    - Parallelization
    - Load balancing

---

title: MapReduce

- Introduced by Google for indexing the web etc.
- Inspired by `map` and `reduce` functions in Lisp
- Functional approach
    - Higher level functions
    - Immutable data -> independent execution
    - Redundance for fault tolerance

---

title: MapReduce basics

- *Input*: a collection of pairs `(K~~1~~, V~~1~~)`
- *Output*: `(K~~2~~, V~~2~~)`
- `map`
    - For each input pair, it generates a list of intermediate pairs
    - Each intermediate key associated with multiple values
- `reduce`
    - For each intermediate key, it aggregates all the corresponding values into a final result

---

title: MapReduce, pseudo-code

code: py

    def map_f(filename: str) -> [(str, int)]:
        counts = {}
        with open(filename) as f:
                for word in f.read().split():
                counts[word] = 1 + counts.get(word, 0)
        return list(counts.items())

    def reduce_f(item: (str, [int])) -> (str, int):
        key, values = item  # key: a word; values: a list of counts
        return key, sum(values)

---

title: MapReduce, simplistic framework

code: py

    def partition(interm: [(str, int)]) -> [(str, [int])]:
        # interm: results of map phase, chained together
        counts = {}
        for key, val in interm:
                if key in counts:
                counts[key].append(val)
                else:
                counts[key] = [val]
        return list(counts.items())

    def main():
        pool = multiprocessing.Pool(8)  # num_workers
        input_list = glob.glob("*.txt")
        map_responses = pool.map(map_f, input_list)
        chained = sum(map_responses, [])
        partitioned_data = partition(chained)
        reduced_values = pool.map(reduce_f, partitioned_data)
        print(list(reduced_values))

---

title: MapReduce - Example [26]

![](images/scipy/mapreduce-example.png)

---

title: MapReduce - map task

- If `map` is a pure function, it can run in parallel on multiple parts of the input
- Input is divided into many blocks (e.g. of 64MB)
- Each assigned to a worker node
- Number of map tasks usually called `M`

---

title: MapReduce - reduce task

- The space of intermediate keys is partitioned, before applying `reduce`
- Each part is a `reduce` task, assigned to a worker node
- Numer of reduce tasks: `R`
- E.g. partitioning policy: `hash(k2) mod R`

---

title: MapReduce - DFS (1)

- Computing model of MapReduce associated with distributed filesystem (DFS)
- Each input file divided into blocks of fixed size
- Each block stored on (more than) a data node
    - Their position stored on a name node (responsible for the namespace)
- Various replication policies of blocks on data nodes
    - Fault tolerance (wrt failures of data nodes) and efficiency

---

title: MapReduce - DFS (2)

- How do MapReduce and DFS combine?
- Each worker is assigned a map task for blocks replicated on that machine (locality)
- Assigned worker writes intermediate keys in its own local node of the DFS
- As many intermediate files, as reduce tasks
- Each reduce task reads from the DFS of each mapper
- It produces a file in local node of DFS, with its own results

---

title: MapReduce - Execution [31]

![](images/scipy/mapreduce-exec.png)

---

title: MapReduce - Fault tolerance

Se un worker fallisce:
- map task completati sono rieseguiti, perché il loro
output non è più accessibile (si trova nel filesyste locale)
- reduce task completati non sono rieseguiti, perché il
loro output è stato salvato nel DFS (quindi i blocchi si
trovano, possibilmente replicati, su altri nodi)

---

title: MapReduce -- alcune ottimizzazioni

combiner:
-- Effettuano una riduzione parziale localmente a ciascun map task.
In molti casi, il combiner è banalmente il reducer.
backup task:
-- Talvolta un task può richiedere molto più tempo del previsto
(per diversi ragioni): quando il processo di MapReduce è
prossimo al completamento, il master avvia delle copie di
backup dei task ancora running, per evitare che il processo sia
rallentato da questi task che sono più lenti del normale.

---

title: MapReduce -- algoritmi iterativi

MapReduce non è particolarmente indicato per
implementare algoritmi iterativi su grafi:
-- Occorre gestire (fuori dal framework) una sequenza di job
MapReduce (sconvenienza)
-- La natura funzionale del modello computazione ci costringe
spesso a copiare l'intero grafo da un'iterazione all'altra
(inefficienza)

---

title: Apache Hadoop

L'implementazione di MapReduce fatta da Google è
proprietaria, ed usata internamente dal motore di ricerca di
Mountain View.
Gli altri possono usare il "clone open-source" Apache
Hadoop.
http://hadoop.apache.org/

---

title: Apache Hadoop 2

Nella versione 2, l'introduzione di YARN per la gestione del cluster, ha permesso ad
Hadoop di svincolarsi da MapReduce (quale unico programming model)
https://it.hortonworks.com/blog/apache-hadoop-2-is-ga/

---

title: HDFS - Assunzioni

Hadoop Distributed Filesystem (HDFS) è il filesystem distribuito alla base di Apache
Hadoop.
Principali assunzioni:
- Hardware failure: tolleranza ai guasti implementata al livello applicativo
(principalmente attraverso la replicazione dei blocchi di dati)
- Streaming Data Access: orientato alle applicazioni batch, piuttosto che all'uso
interattivo. Favorisce il throughput alla latency
- Large Data Sets: decine di milioni di file da GB a TB. Scalabilità a centinaia di nodi
- Simple Coherency Model: una volta che un file è stato chiuso, è solo possibile
appendervi del contenuto oppure troncarlo
- "Moving Computation is Cheaper than Moving Data": HDFS fornisce delle
interfacce che permettono alle applicazioni di spostarsi vicino a dove i dati sono
effettivamente immagazzinati
- Portability across Heterogeneous Hardware and Software Platforms

---

title: HDFS - Architettura

http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html

---

title: HDFS - Cartoon

https://wiki.scc.kit.edu/gridkaschool/upload/1/18/Hdfs-cartoon.pdf

---

title: YARN - Architettura

https://developer.ibm.com/tutorials/bd-yarn-intro/

---

title: Apache Spark

- Cluster computing framework that uses in-memory primitives
- Enable programs to run up to a hundred times faster than Hadoop MapReduce applications
- Spark applications
    - Driver program, controlling ...
    - Execution of parallel operations across a cluster
- Main programming abstraction: Resilient Distributed Datasets (RDDs)
    - Collections of elements partitioned across the nodes of the cluster
    - Can be operated on in parallel

---

title: PySpark

PySpark is Spark’s Python API. PySpark allows Spark applications to
be created from an interactive shell or from Python programs.
Before executing any code within Spark, the application must create
a SparkContext object. The SparkContext object tells Spark how and
where to access a cluster. The master property is a cluster URL that
determines where the Spark appliction will run. The most common
values for master are:
local
Run Spark with one worker thread.
local[n]
Run Spark with n worker threads.
spark://HOST:PORT
Connect to a Spark standalone cluster.
mesos://HOST:PORT
Connect to a Mesos cluster.

---

title: Interactive Shell

In the Spark shell, the SparkContext is created when the shell
launches. The SparkContext is held in the variable sc . The master
PySpark
|
43for the interactive shell can be set by using the --master argument
when the shell is launched. To start an interactive shell, run the
pyspark command:
$ pyspark --master local[4]
...
Welcome to
____
__
/ __/__ ___ _____/ /__
_\ \/ _ \/ _ `/ __/ '_/
/__ / .__/\_,_/_/ /_/\_\
version 1.5.0
/_/
Using Python version 2.7.10 (default, Jul 13 2015 12:05:58)
SparkContext available as sc, HiveContext available as sqlCon-
text.
>>>
For a complete list of options, run pyspark --help .

---

title: Self-Contained Applications

Self-contained applications must first create a SparkContext object
before using any Spark methods. The master can be set when the
SparkContext() method is called:
sc = SparkContext(master='local[4]')
To execute self-contained applications, they must be submitted to
the spark-submit script. The spark-submit script contains many
options; to see a complete listing, run spark-submit --help from
the command line:
$ spark-submit --master local spark_app.py

---

title: Resilient Distributed Datasets (RDDs)

Resilient Distributed Datasets (RDDs) are the fundamental pro‐
gramming abstraction in Spark. RDDs are immutable collections of
data, partitioned across machines, that enable operations to be per‐
formed on elements in parallel. RDDs can be constructed in multi‐
ple ways: by parallelizing existing Python collections, by referencing
files in an external storage system such as HDFS, or by applying
transformations to existing RDDs.

---

title: Creating RDDs from Collections

RDDs can be created from a Python collection by calling the Spark
Context.parallelize() method. The elements of the collection are
copied to form a distributed dataset that can be operated on in par‐
allel. The following example creates a parallelized collection from a
Python list:
>>> data = [1, 2, 3, 4, 5]
>>> rdd = sc.parallelize(data)
>>> rdd.glom().collect()
...
[[1, 2, 3, 4, 5]]
The RDD.glom() method returns a list of all of the elements within
each partition, and the RDD.collect() method brings all the ele‐
ments to the driver node. The result, [[1, 2, 3, 4, 5]] , is the
original collection within a list.
To specify the number of partitions an RDD should be created with,
a second argument can be passed to the parallelize() method.
The following example creates an RDD from the same Python col‐
lection in the previous example, except this time four partitions are
created:
>>> rdd = sc.parallelize(data, 4)
>>> rdd.glom().collect()
...
[[1], [2], [3], [4, 5]]
Using the glom() and collect() methods, the RDD created in this
example contains four inner lists: [1] , [2] , [3] , and [4, 5] . The
number of inner lists represents the number of partitions within the
RDD.

---

title: Creating RDDs from External Sources

RDDs can also be created from files using the SparkContext.text
File() method. Spark can read files residing on the local filesystem,
any storage source supported by Hadoop, Amazon S3, and so on.
Spark supports text files, SequenceFiles, any other Hadoop Input‐
Format, directories, compressed files, and wildcards, e.g., my/direc‐
tory/*.txt. The following example creates a distributed dataset from a
file located on the local filesystem:
Resilient Distributed Datasets (RDDs)
|
45>>> distFile = sc.textFile('data.txt')
>>> distFile.glom().collect()
...
[[u'jack be nimble', u'jack be quick', u'jack jumped over the
candlestick']]
As before, the glom() and collect() methods allow the RDD to be
displayed in its partitions. This result shows that distFile only has
a single partition.
Similar to the parallelize() method, the textFile() method
takes a second parameter that specifies the number of partitions to
create. The following example creates an RDD with three partitions
from the input file:
>>> distFile = sc.textFile('data.txt', 3)
>>> distFile.glom().collect()
...
[[u'jack be nimble', u'jack be quick'], [u'jack jumped over
the candlestick'], []]

---

title: RDD Operations

RDDs support two types of operations: transformations and actions.
Transformations create new datasets from existing ones, and actions
run a computation on the dataset and return results to the driver
program.
Transformations are lazy: that is, their results are not computed
immediately. Instead, Spark remembers all of the transformations
applied to a base dataset. Transformations are computed when an
action requires a result to be returned to the driver program. This
allows Spark to operate efficiently and only transfer the results of the
transformations before an action.
By default, transformations may be recomputed each time an action
is performed on it. This allows Spark to efficiently utilize memory,
but it may utilize more processing resources if the same transforma‐
tions are constantly being processed. To ensure a transformation is
only computed once, the resulting RDD can be persisted in memory
using the RDD.cache() method.

---

title: RDD Workflow

The general workflow for working with RDDs is as follows:
1. Create an RDD from a data source.
2. Apply transformations to an RDD.
3. Apply actions to an RDD.
The following example uses this workflow to calculate the number
of characters in a file:
>>>
>>>
>>>
>>>
59
lines = sc.textFile('data.txt')
line_lengths = lines.map(lambda x: len(x))
document_length = line_lengths.reduce(lambda x,y: x+y)
print document_length
The first statement creates an RDD from the external file data.txt.
This file is not loaded at this point; the variable lines is just a
pointer to the external source. The second statement performs a
transformation on the base RDD by using the map() function to cal‐
culate the number of characters in each line. The variable
line_lengths is not immediately computed due to the laziness of
transformations. Finally, the reduce() method is called, which is an
action. At this point, Spark divides the computations into tasks to
run on separate machines. Each machine runs both the map and
reduction on its local data, returning only the results to the driver
program.
If the application were to use line_lengths again, it would be best
to persist the result of the map transformation to ensure that the
map would not be recomputed. The following line will save
line_lengths into memory after the first time it is computed:
>>> line_lengths.persist()

---

title: Transformations

Transformations create new datasets from existing ones. Lazy evalu‐
ation of transformation allows Spark to remember the set of trans‐
formations applied to the base RDD. This enables Spark to optimize
the required calculations.
This section describes some of Spark’s most common transforma‐
tions. For a full listing of transformations, refer to Spark’s Python
RDD API doc.
map. The map(func) function returns a new RDD by applying a
function, func, to each element of the source. The following example
multiplies each element of the source RDD by two:
>>>
>>>
>>>
>>>
[2,
data = [1, 2, 3, 4, 5, 6]
rdd = sc.parallelize(data)
map_result = rdd.map(lambda x: x * 2)
map_result.collect()
4, 6, 8, 10, 12]
filter. The filter(func) function returns a new RDD containing
only the elements of the source that the supplied function returns as
true. The following example returns only the even numbers from
the source RDD:
>>>
>>>
>>>
[2,
48
|
data = [1, 2, 3, 4, 5, 6]
filter_result = rdd.filter(lambda x: x % 2 == 0)
filter_result.collect()
4, 6]

distinct. The distinct() method returns a new RDD containing
only the distinct elements from the source RDD. The following
example returns the unique elements in a list:
>>>
>>>
>>>
>>>
[4,
data = [1, 2, 3, 2, 4, 1]
rdd = sc.parallelize(data)
distinct_result = rdd.distinct()
distinct_result.collect()
1, 2, 3]

flatMap. The flatMap(func) function is similar to the map() func‐
tion, except it returns a flattened version of the results. For compari‐
son, the following examples return the original element from the
source RDD and its square. The example using the map() function
returns the pairs as a list within a list:
>>> data = [1, 2, 3, 4]
>>> rdd = sc.parallelize(data)
>>> map = rdd.map(lambda x: [x, pow(x,2)])
>>> map.collect()
[[1, 1], [2, 4], [3, 9], [4, 16]]

While the flatMap() function concatenates the results, returning a
single list:
>>>
>>>
>>>
[1,
rdd = sc.parallelize()
flat_map = rdd.flatMap(lambda x: [x, pow(x,2)])
flat_map.collect()
1, 2, 4, 3, 9, 4, 16]

---

title: Actions

Actions cause Spark to compute transformations. After transforms
are computed on the cluster, the result is returned to the driver pro‐
gram.
The following section describes some of Spark’s most common
actions. For a full listing of actions, refer to Spark’s Python RDD API
doc.
reduce. The reduce() method aggregates elements in an RDD
using a function, which takes two arguments and returns one. The
function used in the reduce method is commutative and associative,
ensuring that it can be correctly computed in parallel. The following
example returns the product of all of the elements in the RDD:
>>> data = [1, 2, 3]
>>> rdd = sc.parallelize(data)
Resilient Distributed Datasets (RDDs)
|
49>>> rdd.reduce(lambda a, b: a * b)
6
take. The take(n) method returns an array with the first n ele‐
ments of the RDD. The following example returns the first two ele‐
ments of an RDD:
>>>
>>>
>>>
[1,
data = [1, 2, 3]
rdd = sc.parallelize(data)
rdd.take(2)
2]
collect. The collect() method returns all of the elements of the
RDD as an array. The following example returns all of the elements
from an RDD:
>>>
>>>
>>>
[1,
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)
rdd.collect()
2, 3, 4, 5]
It is important to note that calling collect() on large datasets could
cause the driver to run out of memory. To inspect large RDDs, the
take() and collect() methods can be used to inspect the top n ele‐
ments of a large RDD. The following example will return the first
100 elements of the RDD to the driver:
>>> rdd.take(100).collect()
takeOrdered. The takeOrdered(n, key=func) method returns the
first n elements of the RDD, in their natural order, or as specified by
the function func. The following example returns the first four ele‐
ments of the RDD in descending order:
>>>
>>>
>>>
[6,
data = [6,1,5,2,4,3]
rdd = sc.parallelize(data)
rdd.takeOrdered(4, lambda s: -s)
5, 4, 3]

---

title: HDFS -- Assunzioni
class: break

Hadoop Distributed Filesystem (HDFS) è il filesystem distribuito alla base di Apache
Hadoop.
Principali assunzioni:
- Hardware failure: tolleranza ai guasti implementata al livello applicativo
(principalmente attraverso la replicazione dei blocchi di dati)
- Streaming Data Access: orientato alle applicazioni batch, piuttosto che l'uso
interattivo. Favorisce il throughput alla latency
- Large Data Sets: decine di milioni di file da GB a TB. Scalabilità a centinaia di nodi
- Simple Coherency Model: una volta che un file è stato chiuso, è solo possibile
appendervi del contenuto oppure troncarlo
- "Moving Computation is Cheaper than Moving Data": HDFS fornisce delle
interfacce che permettono alle applicazioni di spostarsi vicino a dove i dati sono
effettivamente immagazzinati
- Portability across Heterogeneous Hardware and Software Platforms

---

title: Hadoop -- Modalità d'uso

- Modalità locale (standalone) [default]
-- Eseguito come un singolo processo Java
-- Utile per il debugging
- Modalità pseudodistribuita
-- Eseguito come un insieme di processi (NameNode, DataNode,
Secondary NameNode, ResourceManager, NodeManager,
WebAppProxy and Map Reduce Job History Server) su una singola
macchina
- Modalità pienamente distribuita
-- I vari processi elencati in precedenza sono distribuiti su più macchine


---

title: Hadoop MapReduce -- InputFormat

Un InputFormat describe la specifica di input per un job
MapReduce.
Il framework MapReduce si affida all'InputFormat del job per:
- Validare la specifica del'input del job.
- Suddividere i file di in istanze di InputSplit, ciascuna delle
quali è assegnato ad un Mapper invididuale.
- Fornire l'implementazione del RecordReader per estrarre
dall'InputSplit i record che saranno elaborate dal Mapper.


---

title: Hadoop MapReduce -- InputSplit

InputSplit rappresenta i dati che devono essere elaborati da un
certo Mapper.
Tipicamente InputSplit presenta un visione dell'input orientata ai
byte, ed è responsabilità del RecordReader elaborarla e
presentare una visione a record.
FileSplit è l'InputSplit predefinito. Esso setta
mapreduce.map.input.file al path del file di input dello split logico.


---

title: Hadoop MapReduce -- RecordReader

Un RecordReader legge coppie <key, value> da un InputSplit.
Tipicamente un RecordReader converte la visione dell'input
orientata ai byte, fornita da un InputSplit, e presenta una
visione a record alle implementazioni di Mapper per
l'elaborazione. Il RecordReader assume pertanto la
responsabilità di elaborare i confini dei record e presentare
ai task chiavi e valori.


---

title: Hadoop MapReduce -- OutputFormat

OutputFormat descrive la specifica di output per un job
MapReduce.
Il framework MapReduce si affida all'OutputFormat del job per:
- Validare la specifica di output del job; per esempio,
controllare la cartella di output non esista già.
- Fornire l'implementazione di RecordWriter usata per scrivere i
file di output del job. I file di output solo immagazzinati in un
FileSystem.
TextOutputFormat è l'OutputFormat predefinito.


---

title: Hadoop MapReduce -- Counter

I counters rappresentano contatori globali, definiti dal
framework MapReduce o dalle applicazioni. Ogni
Counter può essere di qualunque tipo Enum. Contatori
di un particolare Enum sono messi insieme in gruppi di
tipo Counters.Group.

---

title: Hadoop MapReduce -- SkipBadRecords

Hadoop fornisce un'opzione dove un certo insieme di
input record cattivi può essere saltato durante la fase di
map. Le applicazioni possono controllare questa feature
attraverso la classe SkipBadRecords.


---

title: Hadoop MapReduce -- input/output multipli

La classe MultipleInputs supporta job MapReduce con più
input path aventi ciascuno un InputFormat e Mapper diversi.
https://hadoop.apache.org/docs/r2.9.2/api/org/apache/hadoo
p/mapreduce/lib/input/MultipleInputs.html
La classe MultipleOutputs semplifica la scrittura su più file di
output.
https://hadoop.apache.org/docs/r2.9.2/api/org/apache/hadoo
p/mapreduce/lib/output/MultipleOutputs.html

---

title: Hadoop MapReduce -- lib
Nel package org.apache.hadoop.mapreduce.lib si trova
una libreria di classi di interesse.

--

title: Hadoop MapReduce -- scrivere soltanto i valori

Per scrivere soltanto I valori si può usare NullWritable
come tipo di output.

---

title: Hadoop -- Modalità locale (default)

Comandi per shell Unix. Per la PowerShell di Windows è sufficiente usare il
backslash
Esegue un JAR.
$ mkdir input
$ cp etc/hadoop/*.xml input
$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-
2.9.2.jar grep input output 'dfs[a-z.]+'
$ cat output/*

---

