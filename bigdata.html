<!--
Google IO 2012 HTML5 Slide Template

Authors: Eric Bidelman <ebidel@gmail.com>
         Luke Mahe <lukem@google.com>

URL: https://code.google.com/p/io-2012-slides
-->
<!DOCTYPE html>
<html>
<head>
  <title>Framework per Big Data</title>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <link rel="stylesheet" media="all" href="theme/css/default.css">
  <link rel="stylesheet" media="all" href="theme/css/tomamic.css">
  <link rel="stylesheet" media="only screen and (max-device-width: 480px)" href="theme/css/phone.css">
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="js/slides" src="js/require-1.0.8.min.js"></script>
</head>
<body style="opacity: 0">

<slides class="layout-widescreen">

<slide class="title-slide segue nobackground">
  <aside class="gdbar"><img src="theme/logo.png"></aside>
  <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
  <h1 data-config-title><!-- populated from slide_config.json --></h1>
  <figure><img src="images/db/overflow.png"></figure>
  <hgroup>
    <h2>Hadoop e (Py)Spark</h2>
    <p>Michele Tomaiuolo<br>Ingegneria dell'Informazione, UniPR</p>
  </hgroup>
</slide>


<slide  >
  
    <hgroup>
      <h2>Volume and velocity</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Big Data often characterized in terms of 5 Vs (+ others)<ul>
<li>Volume, variety, velocity, value, veracity</li>
</ul>
</li>
<li>Challenges to traditional computational architectures<ul>
<li>Handle large quantities of data (<em>volume</em>)</li>
<li>Readily react to their arrival (<em>velocity</em>)</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>A meta-definition</h2>
      <h3></h3>
    </hgroup>
    <article >
      <blockquote>
<p>Big data should be defined at any point in time as «data whose size forces us to look beyond the tried-and-true methods that are prevalent at that time.» <em>(Jacobs, 2009)</em></p>
</blockquote>
<ul>
<li>Meta-definition centered on <em>volume</em><ul>
<li>It ignores other Vs</li>
</ul>
</li>
<li><em>Moving threshold</em>, for a dataset to be defined as “big data”<ul>
<li>It grows together with technological advances</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Big data pathologies</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Transaction processing and data storage are largely solved problems</li>
<li>Data with a traditional RDBMS<ul>
<li>Easier to <em>get in</em> than <em>get out</em></li>
<li>Worst pathologies are related to <strong>data analysis</strong></li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>How much data?</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Nowadays, a dataset is “big”, if<ul>
<li>It cannot be loaded and processed effectively with desktop tools for data analys and visualization</li>
</ul>
</li>
<li>Ex.: dataset of 7 billion records (Jacob, 2009)</li>
<li>It can be saved in a common HDD<ul>
<li>Simple questions (min, max, avg) can be answered quickly, by reading the file sequentially</li>
<li>More complex queries are difficult</li>
</ul>
</li>
<li>With some effort, it can be loaded in a RDMBS<ul>
<li>Analysis through SQL queries is way too slow</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Bounded Dataset vs Unbounded Dataset</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Bounded Dataset<ul>
<li>A finite set of data, already available at the start</li>
</ul>
</li>
<li>Unbounded Dataset<ul>
<li>Data arriving continuously</li>
<li>Often in a “streaming” scenario</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Unbounded Dataset -- 2 domini temporali</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Unbounded dataset characterized by two temporal domains<ul>
<li>Event time: when the event occurs</li>
<li>Computation time: when data enter the processing system</li>
</ul>
</li>
<li>Often, non-constant difference between them: skew</li>
<li>When the event time is important, skew can complicate the generation of correct results</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Processing of bounded / unbounded dataset</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Bounded datasets<ul>
<li>Often processed with batch systems; e.g. MapReduce</li>
</ul>
</li>
<li>Unbounded datasets<ul>
<li>Processed in either batch or streaming mode</li>
<li>Data buffered in a windowing logic</li>
<li>Sort of bounded dataset, processed as batch</li>
<li>Incomplete data, if event time is important</li>
<li>A user session can be divided in multiple batches, if longer than a window</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Free lunch is over</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>For a long time, programs have taken advantage from growing hardware performances</li>
<li>Limits to Moore's law<ul>
<li>Physical limits make it hard to increase the transistor density</li>
<li>Heat dissipation make it hard to increase frequency</li>
</ul>
</li>
<li>The free lunch is over (Sutter, 2004)<ul>
<li>Multi-core processors</li>
<li>Multi-threaded programs to exploit hardware parallelism</li>
</ul>
</li>
<li>Concurrent programming is hard<ul>
<li>Deadlock, livelock, starvation, race, etc.</li>
<li>Some algorithms are intrinsically sequential</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Amdahl's law</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>A limit to program speedup (how much it is faster on a parallel hardware)<ul>
<li><em>T<sub>1</sub> = T<sub>ser</sub> + T<sub>par</sub></em></li>
<li><em>f = T<sub>ser</sub>/T<sub>1</sub></em></li>
<li><em>T<sub>P</sub> ≥ T<sub>ser</sub> + T<sub>par</sub>/P = fT<sub>1</sub> + (1−f)T<sub>1</sub>/P</em></li>
<li><em>S<sub>P</sub> = T<sub>1</sub>/T<sub>P</sub> ≤ T<sub>1</sub>/(fT<sub>1</sub> + (1−f)T<sub>1</sub>/P) = 1/(f + (1−f)/P)</em></li>
<li><em>S<sub>∞</sub> ≤ lim<sub>P→∞</sub> 1/(f + (1−f)/P) = 1/f</em></li>
</ul>
</li>
<li><em>f</em>: fraction of work which is sequential</li>
<li>A program spending 1% of its execution time on a sequential task<ul>
<li>Speedup limited to max 100 times faster, with unlimited parallelism</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Distributed systems</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Limited resources on a single node -&gt; more nodes</li>
<li>Additional problems, wrt parallel systems<ul>
<li>Is a node dead or very slow?</li>
<li>Is a task being executed?</li>
<li>How to reach consensus?</li>
</ul>
</li>
<li>Framework for distributed computing<ul>
<li>Fault tolerance</li>
<li>Data distribution</li>
<li>Parallelization</li>
<li>Load balancing</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Introduced by Google for indexing the web etc.</li>
<li>Inspired by <code>map</code> and <code>reduce</code> functions in Lisp</li>
<li>Functional approach<ul>
<li>Higher level functions</li>
<li>Immutable data -&gt; independent execution</li>
<li>Redundance for fault tolerance</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce basics</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li><em>Input</em>: a collection of pairs <code>(K<sub>1</sub>, V<sub>1</sub>)</code></li>
<li><em>Output</em>: <code>(K<sub>2</sub>, V<sub>2</sub>)</code></li>
<li><code>map</code><ul>
<li>For each input pair, it generates a list of intermediate pairs</li>
<li>Each intermediate key associated with multiple values</li>
</ul>
</li>
<li><code>reduce</code><ul>
<li>For each intermediate key, it aggregates all the corresponding values into a final result</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce, pseudo-code</h2>
      <h3></h3>
    </hgroup>
    <article >
      <pre class="prettyprint lang-py" data-lang="py"><code>def map_f(filename: str) -&gt; [(str, int)]:
    counts = {}
    with open(filename) as f:
        for word in f.read().split():
            counts[word] = 1 + counts.get(word, 0)
    return list(counts.items())

def reduce_f(item: (str, [int])) -&gt; (str, int):
    key, values = item  # key: a word; values: a list of counts
    return key, sum(values)
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce, simplistic framework</h2>
      <h3></h3>
    </hgroup>
    <article >
      <pre class="prettyprint lang-py" data-lang="py"><code>def partition(interm: [(str, int)]) -&gt; [(str, [int])]:
    # interm: results of map phase, chained together
    counts = {}
    for key, val in interm:
        if key in counts:
            counts[key].append(val)
        else:
            counts[key] = [val]
    return list(counts.items())

def main():
    pool = multiprocessing.Pool(8)  # num_workers
    input_list = glob.glob("*.txt")
    map_responses = pool.map(map_f, input_list)
    chained = sum(map_responses, [])
    partitioned_data = partition(chained)
    reduced_values = pool.map(reduce_f, partitioned_data)
    print(list(reduced_values))
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce -- esempio (2/2) [26]</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>(the, 1)
the cat
the dog
a cat
a dog
(cat, 1) (a, 1)
(the, 1) (cat, 1)
(dog, 1) (cat, 1)
(a, 1) (dog, 1)
(cat, 1)
(a, 1)
(dog, 1)</p>
<p>(a, 1)
(a, 2)
(cat, 2)
(dog, 2)
(dog, 1)
(the, 1)
(the, 2)
(the, 1)</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce -- map task</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>If <code>map</code> is a pure function, it can run in parallel on multiple parts of the input</li>
<li>Input is divided into many blocks (e.g. of 64MB)</li>
<li>Each assigned to a worker node</li>
<li>Number of map tasks usually called <code>M</code></li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce -- reduce task</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>The space of intermediate keys is partitioned, before applying <code>reduce</code></li>
<li>Each part is a <code>reduce</code> task, assigned to a worker node</li>
<li>Numer of reduce tasks: <code>R</code></li>
<li>E.g. partitioning policy: <code>hash(k2) mod R</code></li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce -- DFS (1)</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Computing model of MapReduce associated with distributed filesystem (DFS)</li>
<li>Each input file divided into blocks of fixed size</li>
<li>Each block stored on (more than) a data node<ul>
<li>Their position stored on a name node (responsible for the namespace)</li>
</ul>
</li>
<li>Various replication policies of blocks on data nodes<ul>
<li>Fault tolerance (wrt failures of data nodes) and efficiency</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce -- DFS (2)</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>How do MapReduce and DFS combine?</li>
<li>Each worker is assigned a map task for blocks replicated on that machine (locality)</li>
<li>Assigned worker writes intermediate keys in its own local node of the DFS</li>
<li>As many intermediate files, as reduce tasks</li>
<li>Each reduce task reads from the DFS of each mapper</li>
<li>It produces a file in local node of DFS, with its own results</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce -- esecuzione [31]</h2>
      <h3></h3>
    </hgroup>
    <article >
      </article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce -- fault tolerance</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Se un worker fallisce:
- map task completati sono rieseguiti, perché il loro
output non è più accessibile (si trova nel filesyste locale)
- reduce task completati non sono rieseguiti, perché il
loro output è stato salvato nel DFS (quindi i blocchi si
trovano, possibilmente replicati, su altri nodi)</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce -- alcune ottimizzazioni</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>combiner:
-- Effettuano una riduzione parziale localmente a ciascun map task.
In molti casi, il combiner è banalmente il reducer.
backup task:
-- Talvolta un task può richiedere molto più tempo del previsto
(per diversi ragioni): quando il processo di MapReduce è
prossimo al completamento, il master avvia delle copie di
backup dei task ancora running, per evitare che il processo sia
rallentato da questi task che sono più lenti del normale.</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce -- algoritmi iterativi</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>MapReduce non è particolarmente indicato per
implementare algoritmi iterativi su grafi:
-- Occorre gestire (fuori dal framework) una sequenza di job
MapReduce (sconvenienza)
-- La natura funzionale del modello computazione ci costringe
spesso a copiare l'intero grafo da un'iterazione all'altra
(inefficienza)</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Apache Hadoop</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>L'implementazione di MapReduce fatta da Google è
proprietaria, ed usata internamente dal motore di ricerca di
Mountain View.
Gli altri possono usare il "clone open-source" Apache
Hadoop.
http://hadoop.apache.org/</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Apache Hadoop 2</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Nella versione 2, l'introduzione di YARN per la gestione del cluster, ha permesso ad
Hadoop di svincolarsi da MapReduce (quale unico programming model)
https://it.hortonworks.com/blog/apache-hadoop-2-is-ga/</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>HDFS - Assunzioni</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Hadoop Distributed Filesystem (HDFS) è il filesystem distribuito alla base di Apache
Hadoop.
Principali assunzioni:
- Hardware failure: tolleranza ai guasti implementata al livello applicativo
(principalmente attraverso la replicazione dei blocchi di dati)
- Streaming Data Access: orientato alle applicazioni batch, piuttosto che all'uso
interattivo. Favorisce il throughput alla latency
- Large Data Sets: decine di milioni di file da GB a TB. Scalabilità a centinaia di nodi
- Simple Coherency Model: una volta che un file è stato chiuso, è solo possibile
appendervi del contenuto oppure troncarlo
- "Moving Computation is Cheaper than Moving Data": HDFS fornisce delle
interfacce che permettono alle applicazioni di spostarsi vicino a dove i dati sono
effettivamente immagazzinati
- Portability across Heterogeneous Hardware and Software Platforms</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>HDFS - Architettura</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>HDFS - Cartoon</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>https://wiki.scc.kit.edu/gridkaschool/upload/1/18/Hdfs-cartoon.pdf</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>YARN - Architettura</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>https://developer.ibm.com/tutorials/bd-yarn-intro/</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>HDFS -- Assunzioni</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Hadoop Distributed Filesystem (HDFS) è il filesystem distribuito alla base di Apache
Hadoop.
Principali assunzioni:
- Hardware failure: tolleranza ai guasti implementata al livello applicativo
(principalmente attraverso la replicazione dei blocchi di dati)
- Streaming Data Access: orientato alle applicazioni batch, piuttosto che l'uso
interattivo. Favorisce il throughput alla latency
- Large Data Sets: decine di milioni di file da GB a TB. Scalabilità a centinaia di nodi
- Simple Coherency Model: una volta che un file è stato chiuso, è solo possibile
appendervi del contenuto oppure troncarlo
- "Moving Computation is Cheaper than Moving Data": HDFS fornisce delle
interfacce che permettono alle applicazioni di spostarsi vicino a dove i dati sono
effettivamente immagazzinati
- Portability across Heterogeneous Hardware and Software Platforms</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Hadoop -- Modailità d'uso</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Modalità locale (standalone) [default]
-- Eseguito come un singolo processo Java
-- Utile per il debugging</li>
<li>Modalità pseudodistribuita
-- Eseguito come un insieme di processi (NameNode, DataNode,
Secondary NameNode, ResourceManager, NodeManager,
WebAppProxy and Map Reduce Job History Server) su una singola
macchina</li>
<li>Modalità pienamente distribuita
-- I vari processi elencati in precedenza sono distribuiti su più macchine</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Hadoop MapReduce -- InputFormat</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Un InputFormat describe la specifica di input per un job
MapReduce.
Il framework MapReduce si affida all'InputFormat del job per:
- Validare la specifica del'input del job.
- Suddividere i file di in istanze di InputSplit, ciascuna delle
quali è assegnato ad un Mapper invididuale.
- Fornire l'implementazione del RecordReader per estrarre
dall'InputSplit i record che saranno elaborate dal Mapper.</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Hadoop MapReduce -- InputSplit</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>InputSplit rappresenta i dati che devono essere elaborati da un
certo Mapper.
Tipicamente InputSplit presenta un visione dell'input orientata ai
byte, ed è responsabilità del RecordReader elaborarla e
presentare una visione a record.
FileSplit è l'InputSplit predefinito. Esso setta
mapreduce.map.input.file al path del file di input dello split logico.</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Hadoop MapReduce -- RecordReader</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Un RecordReader legge coppie <key, value> da un InputSplit.
Tipicamente un RecordReader converte la visione dell'input
orientata ai byte, fornita da un InputSplit, e presenta una
visione a record alle implementazioni di Mapper per
l'elaborazione. Il RecordReader assume pertanto la
responsabilità di elaborare i confini dei record e presentare
ai task chiavi e valori.</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Hadoop MapReduce -- OutputFormat</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>OutputFormat descrive la specifica di output per un job
MapReduce.
Il framework MapReduce si affida all'OutputFormat del job per:
- Validare la specifica di output del job; per esempio,
controllare la cartella di output non esista già.
- Fornire l'implementazione di RecordWriter usata per scrivere i
file di output del job. I file di output solo immagazzinati in un
FileSystem.
TextOutputFormat è l'OutputFormat predefinito.</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Hadoop MapReduce -- Counter</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>I counters rappresentano contatori globali, definiti dal
framework MapReduce o dalle applicazioni. Ogni
Counter può essere di qualunque tipo Enum. Contatori
di un particolare Enum sono messi insieme in gruppi di
tipo Counters.Group.</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Hadoop MapReduce -- SkipBadRecords</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Hadoop fornisce un'opzione dove un certo insieme di
input record cattivi può essere saltato durante la fase di
map. Le applicazioni possono controllare questa feature
attraverso la classe SkipBadRecords.</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Hadoop MapReduce -- input/output multipli</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>La classe MultipleInputs supporta job MapReduce con più
input path aventi ciascuno un InputFormat e Mapper diversi.
https://hadoop.apache.org/docs/r2.9.2/api/org/apache/hadoo
p/mapreduce/lib/input/MultipleInputs.html
La classe MultipleOutputs semplifica la scrittura su più file di
output.
https://hadoop.apache.org/docs/r2.9.2/api/org/apache/hadoo
p/mapreduce/lib/output/MultipleOutputs.html</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Hadoop MapReduce -- lib</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>--</p>
<p>title: Hadoop MapReduce -- scrivere soltanto i valori</p>
<p>Per scrivere soltanto I valori si può usare NullWritable
come tipo di output.</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Hadoop -- Modalità locale (default)</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Comandi per shell Unix. Per la PowerShell di Windows è sufficiente usare il
backslash
Esegue un JAR.
$ mkdir input
$ cp etc/hadoop/<em>.xml input
$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-
2.9.2.jar grep input output 'dfs[a-z.]+'
$ cat output/</em></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2></h2>
      <h3></h3>
    </hgroup>
    <article >
      </article>
 
</slide>


<slide class="thank-you-slide segue nobackground">
  <aside class="gdbar right"><img src="theme/logo.png"></aside>
  <article class="flexbox vleft auto-fadein">
    <h2>&lt;Domande?&gt;</h2>
  </article>
  <p class="auto-fadein" data-config-contact>
    Michele Tomaiuolo
    <br>
    Palazzina 1, int. 5708
    <br>
    Ingegneria dell'Informazione, UniPR
    <br>
    <a href="http://sowide.unipr.it/tomamic">sowide.unipr.it/tomamic</a>
  </p>
</slide>

<slide class="backdrop"></slide>

</slides>

</body>
</html>