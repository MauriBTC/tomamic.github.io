<!--
Google IO 2012 HTML5 Slide Template

Authors: Eric Bidelman <ebidel@gmail.com>
         Luke Mahe <lukem@google.com>

URL: https://code.google.com/p/io-2012-slides
-->
<!DOCTYPE html>
<html>
<head>
  <title>Frameworks for Big Data</title>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <link rel="stylesheet" media="all" href="theme/css/default.css">
  <link rel="stylesheet" media="all" href="theme/css/tomamic.css">
  <link rel="stylesheet" media="only screen and (max-device-width: 480px)" href="theme/css/phone.css">
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="js/slides" src="js/require-1.0.8.min.js"></script>
</head>
<body style="opacity: 0">

<slides class="layout-widescreen">

<slide class="title-slide segue nobackground">
  <aside class="gdbar"><img src="theme/logo.png"></aside>
  <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
  <h1 data-config-title><!-- populated from slide_config.json --></h1>
  <figure><img src="images/db/overflow.png"></figure>
  <hgroup>
    <h2>Hadoop & (Py)Spark</h2>
    <p>Michele Tomaiuolo<br>Ingegneria dell'Informazione, UniPR</p>
  </hgroup>
</slide>


<slide  >
  
    <hgroup>
      <h2>‚è© Volume and velocity</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Big Data often characterized in terms of 5 Vs (+ others)<ul>
<li>Volume, variety, velocity, value, veracity</li>
</ul>
</li>
<li>Challenges to traditional computational architectures<ul>
<li>Handle large quantities of data (<em>volume</em>)</li>
<li>Readily react to their arrival (<em>velocity</em>)</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>‚è© A meta-definition</h2>
      <h3></h3>
    </hgroup>
    <article >
      <blockquote>
<p>Big data should be defined at any point in time as ¬´data whose size forces us to look beyond the tried-and-true methods that are prevalent at that time.¬ª <em>(Jacobs, 2009)</em></p>
</blockquote>
<ul>
<li>Meta-definition centered on <em>volume</em><ul>
<li>It ignores other Vs</li>
</ul>
</li>
<li><em>Moving threshold</em>, for a dataset to be defined as ‚Äúbig data‚Äù<ul>
<li>It grows together with technological advances</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>‚è© Big data pathologies</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Transaction processing and data storage are largely solved problems</li>
<li>Data with a traditional RDBMS<ul>
<li>Easier to <em>get in</em> than <em>get out</em></li>
<li>Worst pathologies are related to <strong>data analysis</strong></li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>‚è© How much data?</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Nowadays, a dataset is ‚Äúbig‚Äù, if<ul>
<li>It cannot be loaded and processed effectively with desktop tools for data analys and visualization</li>
</ul>
</li>
<li>Ex.: dataset of 7 billion records (Jacob, 2009)</li>
<li>It can be saved in a common HDD<ul>
<li>Simple questions (min, max, avg) can be answered quickly, by reading the file sequentially</li>
<li>More complex queries are difficult</li>
</ul>
</li>
<li>With some effort, it can be loaded in a RDMBS<ul>
<li>Analysis through SQL queries is way too slow</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>ü§î Bounded vs Unbounded Dataset</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Bounded Dataset<ul>
<li>A finite set of data, already available at the start</li>
</ul>
</li>
<li>Unbounded Dataset<ul>
<li>Data arriving continuously</li>
<li>Often in a ‚Äústreaming‚Äù scenario</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>ü§î Unbounded Dataset - Temporal domain</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Unbounded dataset characterized by two temporal domains<ul>
<li>Event time: when the event occurs</li>
<li>Computation time: when data enter the processing system</li>
</ul>
</li>
<li>Often, non-constant difference between them: skew</li>
<li>When the event time is important, skew can complicate the generation of correct results</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>ü§î Processing of bounded / unbounded dataset</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Bounded datasets<ul>
<li>Often processed with batch systems; e.g. MapReduce</li>
</ul>
</li>
<li>Unbounded datasets<ul>
<li>Processed in either batch or streaming mode</li>
<li>Data buffered in a windowing logic</li>
<li>Sort of bounded dataset, processed as batch</li>
<li>Incomplete data, if event time is important</li>
<li>A user session can be divided in multiple batches, if longer than a window</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Free lunch is over</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>For a long time, programs have taken advantage from growing hardware performances</li>
<li>Limits to Moore's law<ul>
<li>Physical limits make it hard to increase the transistor density</li>
<li>Heat dissipation make it hard to increase frequency</li>
</ul>
</li>
<li><em>‚ÄúThe free lunch is over‚Äù (Sutter, 2004)</em><ul>
<li>Multi-core processors</li>
<li>Multi-threaded programs to exploit hardware parallelism</li>
</ul>
</li>
<li>Concurrent programming is hard<ul>
<li>Deadlock, livelock, starvation, race, etc.</li>
<li>Some algorithms are intrinsically sequential</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Amdahl's law</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>A limit to program speedup (how much it is faster on a parallel hardware)<ul>
<li><em>T<sub>1</sub> = T<sub>ser</sub> + T<sub>par</sub></em></li>
<li><em>f = T<sub>ser</sub>/T<sub>1</sub></em></li>
<li><em>T<sub>P</sub> ‚â• T<sub>ser</sub> + T<sub>par</sub>/P = fT<sub>1</sub> + (1‚àíf)T<sub>1</sub>/P</em></li>
<li><em>S<sub>P</sub> = T<sub>1</sub>/T<sub>P</sub> ‚â§ T<sub>1</sub>/(fT<sub>1</sub> + (1‚àíf)T<sub>1</sub>/P) = 1/(f + (1‚àíf)/P)</em></li>
<li><em>S<sub>‚àû</sub> ‚â§ lim<sub>P‚Üí‚àû</sub> 1/(f + (1‚àíf)/P) = 1/f</em></li>
</ul>
</li>
<li><em>f</em>: fraction of work which is sequential</li>
<li>A program spending 1% of its execution time on a sequential task<ul>
<li>Speedup limited to max 100 times faster, with unlimited parallelism</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Distributed systems</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Limited resources on a single node ‚áí more nodes</li>
<li>Additional problems, wrt parallel systems<ul>
<li>Is a node dead or very slow?</li>
<li>Is a task being executed?</li>
<li>How to reach consensus?</li>
</ul>
</li>
<li>Framework for distributed computing<ul>
<li>Fault tolerance</li>
<li>Data distribution</li>
<li>Parallelization</li>
<li>Load balancing</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Introduced by Google for indexing the web etc.</li>
<li>Inspired by <code>map</code> and <code>reduce</code> functions in Lisp</li>
<li>Functional approach<ul>
<li>Higher level functions</li>
<li>Immutable data ‚áí independent execution</li>
</ul>
</li>
<li>Redundance for fault tolerance</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce basics</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li><em>Input</em>: a collection of pairs <code>(K<sub>1</sub>, V<sub>1</sub>)</code></li>
<li><em>Output</em>: <code>(K<sub>2</sub>, V<sub>2</sub>)</code></li>
<li><code>map</code><ul>
<li>For each input pair, it generates a list of intermediate pairs</li>
<li>Each intermediate key associated with multiple values</li>
</ul>
</li>
<li><code>reduce</code><ul>
<li>For each intermediate key, it aggregates all the corresponding values into a final result</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce, pseudo-code</h2>
      <h3></h3>
    </hgroup>
    <article >
      <pre class="prettyprint lang-py" data-lang="py"><code>def map_f(filename: str) -&gt; [(str, int)]:
    counts = {}
    with open(filename) as f:
        for word in f.read().split():
            counts[word] = 1 + counts.get(word, 0)
    return list(counts.items())

def reduce_f(item: (str, [int])) -&gt; (str, int):
    key, values = item  # key: a word; values: a list of counts
    return key, sum(values)
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce, simplistic framework</h2>
      <h3></h3>
    </hgroup>
    <article >
      <pre class="prettyprint lang-py" data-lang="py"><code>def partition(interm: [(str, int)]) -&gt; [(str, [int])]:
    # interm: results of map phase, chained together
    counts = {}
    for key, val in interm:
        if key in counts: counts[key].append(val)
        else: counts[key] = [val]
    return list(counts.items())

def main():
    pool = multiprocessing.Pool(8)  # num_workers
    input_list = glob.glob("*.txt")
    map_responses = pool.map(map_f, input_list)
    chained = sum(map_responses, [])
    partitioned_data = partition(chained)
    reduced_values = pool.map(reduce_f, partitioned_data)
    print(list(reduced_values))
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce - Example [26]</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p><img alt="" src="images/scipy/mapreduce-example.png" /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>ü§î MapReduce - Map task</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>If <code>map</code> is a pure function, it can run in parallel on multiple parts of the input</li>
<li>Input is divided into many blocks (e.g. of 64MB)</li>
<li>Each assigned to a worker node</li>
<li>Number of map tasks usually called <code>M</code></li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>ü§î MapReduce - Reduce task</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>The space of intermediate keys is partitioned, before applying <code>reduce</code></li>
<li>Each part is a <code>reduce</code> task, assigned to a worker node</li>
<li>Number of reduce tasks: <code>R</code></li>
<li>E.g. partitioning policy: <code>hash(k2) mod R</code></li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>ü§î MapReduce - DFS (1)</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Computing model of MapReduce associated with distributed filesystem (DFS)</li>
<li>Each input file divided into blocks of fixed size</li>
<li>Each block stored on (more than) a data node<ul>
<li>Their position stored on a name node (responsible for the namespace)</li>
</ul>
</li>
<li>Various replication policies of blocks on data nodes<ul>
<li>Fault tolerance (wrt failures of data nodes) and efficiency</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>ü§î MapReduce - DFS (2)</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>How do MapReduce and DFS combine?</li>
<li>Each worker is assigned a map task for blocks replicated on that machine (locality)</li>
<li>Assigned worker writes intermediate keys in its own local node of the DFS</li>
<li>As many intermediate files, as reduce tasks</li>
<li>Each reduce task reads from the DFS of each mapper</li>
<li>It produces a file in local node of DFS, with its own results</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>ü§î MapReduce - Execution [31]</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p><img alt="" src="images/scipy/mapreduce-exec.png" /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Apache Hadoop</h2>
      <h3></h3>
    </hgroup>
    <article >
      <figure>
        <img src="images/scipy/hadoop.png">
        
      </figure><ul>
<li><em>MapReduce</em>: developed by Google<ul>
<li>Proprietary, for internal projects and use</li>
</ul>
</li>
<li><em>Apache Hadoop</em>: open-source<ul>
<li><a href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></li>
<li><em>Hadoop Distributed Filesystem (HDFS)</em></li>
</ul>
</li>
<li>Apache Hadoop 2, <em>YARN</em> resource negotiator<ul>
<li>Cluster management</li>
<li>Overcome MapReduce, as unique programming model</li>
<li><a href="https://it.hortonworks.com/blog/apache-hadoop-2-is-ga/">https://it.hortonworks.com/blog/apache-hadoop-2-is-ga/</a></li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>ü§î HDFS - Main assunptions</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Hadoop Distributed Filesystem (HDFS): at the basis of Apache Hadoop</li>
<li>Hardware failure: fault tollerance at application level (data block replication)</li>
<li>Streaming Data Access: batch-oriented instead of interactive; favours throughput over latency</li>
<li>Large Data Sets: millions GB/TB files; scalability to 100s nodes</li>
<li>Simple Coherency Model: only append data or truncate files</li>
<li>‚ÄúMoving Computation is Cheaper than Moving Data‚Äù</li>
<li>Portability across heterogeneous hardware and software platforms</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>ü§î HDFS - Architecture</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p><img alt="" src="images/scipy/hdfs-architecture.png" /></p>
<blockquote></blockquote>
<p><a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html</a></p></article>
 
</slide>

<slide class="large-image" >
  
    <hgroup>
      <h2>ü§î HDFS - Cartoon 1</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p><img alt="" src="images/scipy/hdfs-cartoon-1.png" /></p>
<blockquote></blockquote>
<p><a href="https://wiki.scc.kit.edu/gridkaschool/upload/1/18/Hdfs-cartoon.pdf">https://wiki.scc.kit.edu/gridkaschool/upload/1/18/Hdfs-cartoon.pdf</a></p></article>
 
</slide>

<slide class="large-image" >
  
    <hgroup>
      <h2>ü§î HDFS - Cartoon 2</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p><img alt="" src="images/scipy/hdfs-cartoon-2.png" /></p>
<blockquote></blockquote>
<p><a href="https://wiki.scc.kit.edu/gridkaschool/upload/1/18/Hdfs-cartoon.pdf">https://wiki.scc.kit.edu/gridkaschool/upload/1/18/Hdfs-cartoon.pdf</a></p></article>
 
</slide>

<slide class="large-image" >
  
    <hgroup>
      <h2>ü§î HDFS - Cartoon 3</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p><img alt="" src="images/scipy/hdfs-cartoon-3.png" /></p>
<blockquote></blockquote>
<p><a href="https://wiki.scc.kit.edu/gridkaschool/upload/1/18/Hdfs-cartoon.pdf">https://wiki.scc.kit.edu/gridkaschool/upload/1/18/Hdfs-cartoon.pdf</a></p></article>
 
</slide>

<slide class="large-image" >
  
    <hgroup>
      <h2>ü§î HDFS - Cartoon 4</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p><img alt="" src="images/scipy/hdfs-cartoon-4.png" /></p>
<blockquote></blockquote>
<p><a href="https://wiki.scc.kit.edu/gridkaschool/upload/1/18/Hdfs-cartoon.pdf">https://wiki.scc.kit.edu/gridkaschool/upload/1/18/Hdfs-cartoon.pdf</a></p></article>
 
</slide>

<slide class="large-image" >
  
    <hgroup>
      <h2>ü§î YARN - Architettura</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p><img alt="" src="images/scipy/yarn-architecture.png" /></p>
<blockquote></blockquote>
<p><a href="https://developer.ibm.com/tutorials/bd-yarn-intro/">https://developer.ibm.com/tutorials/bd-yarn-intro/</a></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Apache Spark</h2>
      <h3></h3>
    </hgroup>
    <article >
      <figure>
        <img src="images/scipy/spark.png">
        
      </figure><ul>
<li>Cluster computing framework that uses <em>in-memory</em> primitives</li>
<li>Enable programs to run up to a hundred times <em>faster</em> than Hadoop MapReduce applications</li>
<li>Spark applications<ul>
<li><em>Driver</em> program, controlling ...</li>
<li>Execution of parallel operations across a cluster</li>
</ul>
</li>
<li>Main programming abstraction: <strong>RDD</strong> <em>(Resilient Distributed Dataset)</em><ul>
<li>Collections of elements partitioned across the nodes of the cluster</li>
<li>Can be operated on in parallel</li>
</ul>
</li>
<li>Main programming language: <em>Scala</em> (JVM)</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>PySpark</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Spark‚Äôs <em>Python API</em><ul>
<li>Spark applications created from interactive shell or Python programs</li>
</ul>
</li>
<li>Before executing any code within Spark, the application must create a <em>SparkContext</em> object<ul>
<li>How and where to access a cluster</li>
</ul>
</li>
<li><em>Master</em> property: cluster URL, where the Spark appliction will run<ul>
<li><code>local</code> -- Run Spark with one worker thread</li>
<li><code>local[n]</code> -- Run Spark with n worker threads</li>
<li><code>spark://HOST:PORT</code> -- Connect to a Spark cluster</li>
<li><code>mesos://HOST:PORT</code> -- Connect to a Mesos cluster</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Local PySpark environment</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li><code>pip3 install pyspark</code></li>
<li>Download latest Spark<ul>
<li><a href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a></li>
<li>Spark 2.4.4 built for Hadoop 2.7.3</li>
</ul>
</li>
<li>Unzip it in ~/spark</li>
</ul>
<pre class="" data-lang="shell"><code>export PYSPARK_PYTHON=python3
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS=notebook
</code></pre>
<ul>
<li>Launch: <code>~/spark/bin/pyspark</code></li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Interactive Shell</h2>
      <h3></h3>
    </hgroup>
    <article >
      <pre class="" data-lang="shell"><code>~$ ~/spark/bin/pyspark
</code></pre>
<pre class="" data-lang="shell"><code>Python 3.7.5 (default, Nov 20 2019, 09:21:52)
[GCC 9.2.1 20191008] on linux
Type "help", "copyright", "credits" or "license" for more information.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/

Using Python version 3.7.5 (default, Nov 20 2019 09:21:52)
SparkSession available as 'spark'.
&gt;&gt;&gt;
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Compute Pi</h2>
      <h3></h3>
    </hgroup>
    <article >
      <pre class="prettyprint lang-py" data-lang="py"><code>##from pyspark import SparkContext
##sc = SparkContext.getOrCreate()

import random
num_samples = 1000000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y &lt; 1

count = sc.parallelize(range(0, num_samples)).filter(inside).count()

pi = 4 * count / num_samples
print(pi)

##sc.stop()
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Resilient Distributed Dataset (RDD)</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Fundamental programming abstraction in Spark</li>
<li>Immutable collection of data, partitioned across machines</li>
<li>Enable operations to be performed on elements in parallel</li>
<li>Constructed in multiple ways<ul>
<li>Parallelizing existing Python collections</li>
<li>Referencing files in an external storage system (e.g., HDFS)</li>
<li>Applying transformations to existing RDDs</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>RDD from a collection</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li><code>SparkContext.parallelize()</code> create a RDD from a Python collection</li>
<li>Elements copied to form a distributed dataset, operated on in parallel</li>
</ul>
<pre class="prettyprint lang-py" data-lang="py"><code>&gt;&gt;&gt; data = [1, 2, 3, 4, 5]
&gt;&gt;&gt; rdd = sc.parallelize(data, 3)
&gt;&gt;&gt; rdd.glom().collect()
[[1], [2, 3], [4, 5]]
</code></pre>
<ul>
<li><code>RDD.glom()</code> returns a list of all partitions</li>
<li><code>RDD.collect()</code> brings all elements to the driver node</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>RDD from external source</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>RDDs can also be created from files using <code>SparkContext.textFile()</code></li>
<li>Files on local filesystem, Hadoop storage, Amazon S3 ...</li>
<li>Text files, SequenceFiles, other Hadoop Input‚ÄêFormat, directories, compressed files, wildcards<ul>
<li>e.g., <code>my/directory/*.txt</code></li>
</ul>
</li>
</ul>
<pre class="prettyprint lang-py" data-lang="py"><code>&gt;&gt;&gt; distFile = sc.textFile("~/spark/README.md", 3)  # 3 partitions
&gt;&gt;&gt; distFile.glom().collect()
[['# Apache Spark', '', 'Spark is a fast and general...'],
['...', '...'], ['...', '...']]
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Transformations and actions</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>RDDs support two types of operations<ul>
<li><strong>Transformations</strong>: create new datasets from existing ones</li>
<li><strong>Actions</strong>: run a computation on the dataset, return results to the driver</li>
</ul>
</li>
<li>Transformations are lazy<ul>
<li>Their results are not computed immediately</li>
</ul>
</li>
<li>Actions trigger actual computation<ul>
<li>When a result has to be returned to the driver program</li>
</ul>
</li>
<li>Transformations may be recomputed<ul>
<li>Each time an action is performed on it</li>
<li><code>RDD.cache()</code>: RDD persisted in memory</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Workflow</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>The general workflow for working with RDDs is as follows<ul>
<li><strong>(1)</strong> Create an RDD from a data source</li>
<li><strong>(2)</strong> Apply transformations to an RDD</li>
<li><strong>(3)</strong> Apply actions to an RDD</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Example workflow</h2>
      <h3></h3>
    </hgroup>
    <article >
      <pre class="prettyprint lang-py" data-lang="py"><code>lines = sc.textFile("~/spark/README.md")
line_lengths = lines.map(lambda x: len(x))
##line_lengths.cache()
document_length = line_lengths.reduce(lambda x,y: x+y)
print(document_length)
</code></pre>
<ul>
<li><strong>(1)</strong> <code>textFile</code>, <em>creation</em>: file is not loaded at this point</li>
<li><strong>(2)</strong> <code>map</code>, <em>transformation</em>: not immediately computed<ul>
<li>If <code>line_lengths</code> to be used again, persist it</li>
</ul>
</li>
<li><strong>(3)</strong> <code>reduce</code>, <em>action</em>: computation is performed<ul>
<li>Spark divides the computations into tasks</li>
<li>Each machine runs both map and reduce on its local data</li>
<li>It returns results to the driver</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Map transformation</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Applying a function to each element of the source RDD</li>
<li>Return a new RDD with results</li>
</ul>
<pre class="prettyprint lang-py" data-lang="py"><code>&gt;&gt;&gt; data = [1, 2, 3, 4, 5, 6]
&gt;&gt;&gt; rdd = sc.parallelize(data)
&gt;&gt;&gt; map_result = rdd.map(lambda x: x * 2)
&gt;&gt;&gt; map_result.collect()
[2, 4, 6, 8, 10, 12]
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Filter transformation</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Applying a function (predicate) to each element of the source RDD</li>
<li>Return a new RDD, containing only elements returning true</li>
</ul>
<pre class="prettyprint lang-py" data-lang="py"><code>&gt;&gt;&gt; data = [1, 2, 3, 4, 5, 6]
&gt;&gt;&gt; filter_result = rdd.filter(lambda x: x % 2 == 0)
&gt;&gt;&gt; filter_result.collect()
[2, 4, 6]
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Distinct transformation</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>New RDD, containing only distinct elements from the source RDD</li>
</ul>
<pre class="prettyprint lang-py" data-lang="py"><code>&gt;&gt;&gt; data = [1, 2, 3, 2, 4, 1]
&gt;&gt;&gt; rdd = sc.parallelize(data)
&gt;&gt;&gt; distinct_result = rdd.distinct()
&gt;&gt;&gt; distinct_result.collect()
[4, 1, 2, 3]
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Flat Map transformation</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Similar to <code>map</code>, but flattened version of results</li>
</ul>
<pre class="prettyprint lang-py" data-lang="py"><code>&gt;&gt;&gt; data = [1, 2, 3, 4]
&gt;&gt;&gt; rdd = sc.parallelize(data)
&gt;&gt;&gt; map_result = rdd.map(lambda x: [x, pow(x, 2)])
&gt;&gt;&gt; map_result.collect()
[[1, 1], [2, 4], [3, 9], [4, 16]]
</code></pre>
<pre class="prettyprint lang-py" data-lang="py"><code>&gt;&gt;&gt; rdd = sc.parallelize()
&gt;&gt;&gt; flat_result = rdd.flatMap(lambda x: [x, pow(x, 2)])
&gt;&gt;&gt; flat_result.collect()
[1, 1, 2, 4, 3, 9, 4, 16]
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Reduce action</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Aggregate elements in an RDD using a function</li>
<li>The function takes two arguments and returns one</li>
<li>The function must be commutative and associative<ul>
<li>Correctly computed in parallel</li>
</ul>
</li>
</ul>
<pre class="prettyprint lang-py" data-lang="py"><code>&gt;&gt;&gt; data = [1, 2, 3]
&gt;&gt;&gt; rdd = sc.parallelize(data)
&gt;&gt;&gt; rdd.reduce(lambda a, b: a * b)
6
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Take action</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Return first n elements of the source RDD</li>
</ul>
<pre class="prettyprint lang-py" data-lang="py"><code>&gt;&gt;&gt; data = [1, 2, 3]
&gt;&gt;&gt; rdd = sc.parallelize(data)
&gt;&gt;&gt; rdd.take(2)
[1, 2]
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Collect action</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Returns all elements of RDD, as an array</li>
</ul>
<pre class="prettyprint lang-py" data-lang="py"><code>&gt;&gt;&gt; data = [1, 2, 3, 4, 5]
&gt;&gt;&gt; rdd = sc.parallelize(data)
&gt;&gt;&gt; rdd.collect()
[1, 2, 3, 4, 5]
</code></pre>
<ul>
<li>Cause the driver to run out of memory</li>
<li>To inspect large RDDs, use <code>take</code> and <code>collect</code></li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Take Ordered action</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li><code>takeOrdered(n, key=func)</code> returns the first <code>n</code> elements of the RDD<ul>
<li>In their natural order, or</li>
<li>As specified by <code>func</code></li>
</ul>
</li>
</ul>
<pre class="prettyprint lang-py" data-lang="py"><code>&gt;&gt;&gt; data = [6,1,5,2,4,3]
&gt;&gt;&gt; rdd = sc.parallelize(data)
&gt;&gt;&gt; rdd.takeOrdered(4, lambda s: -s)
[6, 5, 4, 3]
</code></pre></article>
 
</slide>


<slide class="thank-you-slide segue nobackground">
  <aside class="gdbar right"><img src="theme/logo.png"></aside>
  <article class="flexbox vleft auto-fadein">
    <h2>&lt;Domande?&gt;</h2>
  </article>
  <p class="auto-fadein" data-config-contact>
    Michele Tomaiuolo
    <br>
    Palazzina 1, int. 5708
    <br>
    Ingegneria dell'Informazione, UniPR
    <br>
    <a href="http://sowide.unipr.it/tomamic">sowide.unipr.it/tomamic</a>
  </p>
</slide>

<slide class="backdrop"></slide>

</slides>

</body>
</html>