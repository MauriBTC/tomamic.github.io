<!--
Google IO 2012 HTML5 Slide Template

Authors: Eric Bidelman <ebidel@gmail.com>
         Luke Mahe <lukem@google.com>

URL: https://code.google.com/p/io-2012-slides
-->
<!DOCTYPE html>
<html>
<head>
  <title>Framework per Big Data</title>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <link rel="stylesheet" media="all" href="theme/css/default.css">
  <link rel="stylesheet" media="all" href="theme/css/tomamic.css">
  <link rel="stylesheet" media="only screen and (max-device-width: 480px)" href="theme/css/phone.css">
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="js/slides" src="js/require-1.0.8.min.js"></script>
</head>
<body style="opacity: 0">

<slides class="layout-widescreen">

<slide class="title-slide segue nobackground">
  <aside class="gdbar"><img src="theme/logo.png"></aside>
  <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
  <h1 data-config-title><!-- populated from slide_config.json --></h1>
  <figure><img src="images/db/overflow.png"></figure>
  <hgroup>
    <h2>Hadoop e (Py)Spark</h2>
    <p>Michele Tomaiuolo<br>Ingegneria dell'Informazione, UniPR</p>
  </hgroup>
</slide>


<slide  >
  
    <hgroup>
      <h2>Volume and velocity</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Big Data often characterized in terms of 5 Vs (+ others)<ul>
<li>Volume, variety, velocity, value, veracity</li>
</ul>
</li>
<li>Challenges to traditional computational architectures<ul>
<li>Handle large quantities of data (<em>volume</em>)</li>
<li>Readily react to their arrival (<em>velocity</em>)</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>A meta-definition</h2>
      <h3></h3>
    </hgroup>
    <article >
      <blockquote>
<p>Big data should be defined at any point in time as «data whose size forces us to look beyond the tried-and-true methods that are prevalent at that time.» <em>(Jacobs, 2009)</em></p>
</blockquote>
<ul>
<li>Meta-definition centered on <em>volume</em><ul>
<li>It ignores other Vs</li>
</ul>
</li>
<li><em>Moving threshold</em>, for a dataset to be defined as “big data”<ul>
<li>It grows together with technological advances</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Big data pathologies</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Transaction processing and data storage are largely solved problems</li>
<li>Data with a traditional RDBMS<ul>
<li>Easier to <em>get in</em> than <em>get out</em></li>
<li>Worst pathologies are related to <strong>data analysis</strong></li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>How much data?</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Nowadays, a dataset is “big”, if<ul>
<li>It cannot be loaded and processed effectively with desktop tools for data analys and visualization</li>
</ul>
</li>
<li>Ex.: dataset of 7 billion records (Jacob, 2009)</li>
<li>It can be saved in a common HDD<ul>
<li>Simple questions (min, max, avg) can be answered quickly, by reading the file sequentially</li>
<li>More complex queries are difficult</li>
</ul>
</li>
<li>With some effort, it can be loaded in a RDMBS<ul>
<li>Analysis through SQL queries is way too slow</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Bounded vs Unbounded Dataset</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Bounded Dataset<ul>
<li>A finite set of data, already available at the start</li>
</ul>
</li>
<li>Unbounded Dataset<ul>
<li>Data arriving continuously</li>
<li>Often in a “streaming” scenario</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Unbounded Dataset - Temporal domain</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Unbounded dataset characterized by two temporal domains<ul>
<li>Event time: when the event occurs</li>
<li>Computation time: when data enter the processing system</li>
</ul>
</li>
<li>Often, non-constant difference between them: skew</li>
<li>When the event time is important, skew can complicate the generation of correct results</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Processing of bounded / unbounded dataset</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Bounded datasets<ul>
<li>Often processed with batch systems; e.g. MapReduce</li>
</ul>
</li>
<li>Unbounded datasets<ul>
<li>Processed in either batch or streaming mode</li>
<li>Data buffered in a windowing logic</li>
<li>Sort of bounded dataset, processed as batch</li>
<li>Incomplete data, if event time is important</li>
<li>A user session can be divided in multiple batches, if longer than a window</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Free lunch is over</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>For a long time, programs have taken advantage from growing hardware performances</li>
<li>Limits to Moore's law<ul>
<li>Physical limits make it hard to increase the transistor density</li>
<li>Heat dissipation make it hard to increase frequency</li>
</ul>
</li>
<li><em>“The free lunch is over” (Sutter, 2004)</em><ul>
<li>Multi-core processors</li>
<li>Multi-threaded programs to exploit hardware parallelism</li>
</ul>
</li>
<li>Concurrent programming is hard<ul>
<li>Deadlock, livelock, starvation, race, etc.</li>
<li>Some algorithms are intrinsically sequential</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Amdahl's law</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>A limit to program speedup (how much it is faster on a parallel hardware)<ul>
<li><em>T<sub>1</sub> = T<sub>ser</sub> + T<sub>par</sub></em></li>
<li><em>f = T<sub>ser</sub>/T<sub>1</sub></em></li>
<li><em>T<sub>P</sub> ≥ T<sub>ser</sub> + T<sub>par</sub>/P = fT<sub>1</sub> + (1−f)T<sub>1</sub>/P</em></li>
<li><em>S<sub>P</sub> = T<sub>1</sub>/T<sub>P</sub> ≤ T<sub>1</sub>/(fT<sub>1</sub> + (1−f)T<sub>1</sub>/P) = 1/(f + (1−f)/P)</em></li>
<li><em>S<sub>∞</sub> ≤ lim<sub>P→∞</sub> 1/(f + (1−f)/P) = 1/f</em></li>
</ul>
</li>
<li><em>f</em>: fraction of work which is sequential</li>
<li>A program spending 1% of its execution time on a sequential task<ul>
<li>Speedup limited to max 100 times faster, with unlimited parallelism</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Distributed systems</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Limited resources on a single node -&gt; more nodes</li>
<li>Additional problems, wrt parallel systems<ul>
<li>Is a node dead or very slow?</li>
<li>Is a task being executed?</li>
<li>How to reach consensus?</li>
</ul>
</li>
<li>Framework for distributed computing<ul>
<li>Fault tolerance</li>
<li>Data distribution</li>
<li>Parallelization</li>
<li>Load balancing</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Introduced by Google for indexing the web etc.</li>
<li>Inspired by <code>map</code> and <code>reduce</code> functions in Lisp</li>
<li>Functional approach<ul>
<li>Higher level functions</li>
<li>Immutable data -&gt; independent execution</li>
<li>Redundance for fault tolerance</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce basics</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li><em>Input</em>: a collection of pairs <code>(K<sub>1</sub>, V<sub>1</sub>)</code></li>
<li><em>Output</em>: <code>(K<sub>2</sub>, V<sub>2</sub>)</code></li>
<li><code>map</code><ul>
<li>For each input pair, it generates a list of intermediate pairs</li>
<li>Each intermediate key associated with multiple values</li>
</ul>
</li>
<li><code>reduce</code><ul>
<li>For each intermediate key, it aggregates all the corresponding values into a final result</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce, pseudo-code</h2>
      <h3></h3>
    </hgroup>
    <article >
      <pre class="prettyprint lang-py" data-lang="py"><code>def map_f(filename: str) -&gt; [(str, int)]:
    counts = {}
    with open(filename) as f:
            for word in f.read().split():
            counts[word] = 1 + counts.get(word, 0)
    return list(counts.items())

def reduce_f(item: (str, [int])) -&gt; (str, int):
    key, values = item  # key: a word; values: a list of counts
    return key, sum(values)
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce, simplistic framework</h2>
      <h3></h3>
    </hgroup>
    <article >
      <pre class="prettyprint lang-py" data-lang="py"><code>def partition(interm: [(str, int)]) -&gt; [(str, [int])]:
    # interm: results of map phase, chained together
    counts = {}
    for key, val in interm:
            if key in counts:
            counts[key].append(val)
            else:
            counts[key] = [val]
    return list(counts.items())

def main():
    pool = multiprocessing.Pool(8)  # num_workers
    input_list = glob.glob("*.txt")
    map_responses = pool.map(map_f, input_list)
    chained = sum(map_responses, [])
    partitioned_data = partition(chained)
    reduced_values = pool.map(reduce_f, partitioned_data)
    print(list(reduced_values))
</code></pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce - Example [26]</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p><img alt="" src="images/scipy/mapreduce-example.png" /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce - map task</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>If <code>map</code> is a pure function, it can run in parallel on multiple parts of the input</li>
<li>Input is divided into many blocks (e.g. of 64MB)</li>
<li>Each assigned to a worker node</li>
<li>Number of map tasks usually called <code>M</code></li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce - reduce task</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>The space of intermediate keys is partitioned, before applying <code>reduce</code></li>
<li>Each part is a <code>reduce</code> task, assigned to a worker node</li>
<li>Numer of reduce tasks: <code>R</code></li>
<li>E.g. partitioning policy: <code>hash(k2) mod R</code></li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce - DFS (1)</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Computing model of MapReduce associated with distributed filesystem (DFS)</li>
<li>Each input file divided into blocks of fixed size</li>
<li>Each block stored on (more than) a data node<ul>
<li>Their position stored on a name node (responsible for the namespace)</li>
</ul>
</li>
<li>Various replication policies of blocks on data nodes<ul>
<li>Fault tolerance (wrt failures of data nodes) and efficiency</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce - DFS (2)</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>How do MapReduce and DFS combine?</li>
<li>Each worker is assigned a map task for blocks replicated on that machine (locality)</li>
<li>Assigned worker writes intermediate keys in its own local node of the DFS</li>
<li>As many intermediate files, as reduce tasks</li>
<li>Each reduce task reads from the DFS of each mapper</li>
<li>It produces a file in local node of DFS, with its own results</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce - Execution [31]</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p><img alt="" src="images/scipy/mapreduce-exec.png" /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce - Fault tolerance</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Se un worker fallisce:
- map task completati sono rieseguiti, perché il loro
output non è più accessibile (si trova nel filesyste locale)
- reduce task completati non sono rieseguiti, perché il
loro output è stato salvato nel DFS (quindi i blocchi si
trovano, possibilmente replicati, su altri nodi)</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce -- alcune ottimizzazioni</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>combiner:
-- Effettuano una riduzione parziale localmente a ciascun map task.
In molti casi, il combiner è banalmente il reducer.
backup task:
-- Talvolta un task può richiedere molto più tempo del previsto
(per diversi ragioni): quando il processo di MapReduce è
prossimo al completamento, il master avvia delle copie di
backup dei task ancora running, per evitare che il processo sia
rallentato da questi task che sono più lenti del normale.</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MapReduce -- algoritmi iterativi</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>MapReduce non è particolarmente indicato per
implementare algoritmi iterativi su grafi:
-- Occorre gestire (fuori dal framework) una sequenza di job
MapReduce (sconvenienza)
-- La natura funzionale del modello computazione ci costringe
spesso a copiare l'intero grafo da un'iterazione all'altra
(inefficienza)</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Apache Hadoop</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>L'implementazione di MapReduce fatta da Google è
proprietaria, ed usata internamente dal motore di ricerca di
Mountain View.
Gli altri possono usare il "clone open-source" Apache
Hadoop.
http://hadoop.apache.org/</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Apache Hadoop 2</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Nella versione 2, l'introduzione di YARN per la gestione del cluster, ha permesso ad
Hadoop di svincolarsi da MapReduce (quale unico programming model)
https://it.hortonworks.com/blog/apache-hadoop-2-is-ga/</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>HDFS - Assunzioni</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Hadoop Distributed Filesystem (HDFS) è il filesystem distribuito alla base di Apache
Hadoop.
Principali assunzioni:
- Hardware failure: tolleranza ai guasti implementata al livello applicativo
(principalmente attraverso la replicazione dei blocchi di dati)
- Streaming Data Access: orientato alle applicazioni batch, piuttosto che all'uso
interattivo. Favorisce il throughput alla latency
- Large Data Sets: decine di milioni di file da GB a TB. Scalabilità a centinaia di nodi
- Simple Coherency Model: una volta che un file è stato chiuso, è solo possibile
appendervi del contenuto oppure troncarlo
- "Moving Computation is Cheaper than Moving Data": HDFS fornisce delle
interfacce che permettono alle applicazioni di spostarsi vicino a dove i dati sono
effettivamente immagazzinati
- Portability across Heterogeneous Hardware and Software Platforms</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>HDFS - Architettura</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>HDFS - Cartoon</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>https://wiki.scc.kit.edu/gridkaschool/upload/1/18/Hdfs-cartoon.pdf</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>YARN - Architettura</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>https://developer.ibm.com/tutorials/bd-yarn-intro/</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Apache Spark</h2>
      <h3></h3>
    </hgroup>
    <article >
      <ul>
<li>Cluster computing framework that uses in-memory primitives</li>
<li>Enable programs to run up to a hundred times faster than Hadoop MapReduce applications</li>
<li>Spark applications<ul>
<li>Driver program, controlling ...</li>
<li>Execution of parallel operations across a cluster</li>
</ul>
</li>
<li>Main programming abstraction: Resilient Distributed Datasets (RDDs)<ul>
<li>Collections of elements partitioned across the nodes of the cluster</li>
<li>Can be operated on in parallel</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>PySpark</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>PySpark is Spark’s Python API. PySpark allows Spark applications to
be created from an interactive shell or from Python programs.
Before executing any code within Spark, the application must create
a SparkContext object. The SparkContext object tells Spark how and
where to access a cluster. The master property is a cluster URL that
determines where the Spark appliction will run. The most common
values for master are:
local
Run Spark with one worker thread.
local[n]
Run Spark with n worker threads.
spark://HOST:PORT
Connect to a Spark standalone cluster.
mesos://HOST:PORT
Connect to a Mesos cluster.</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Interactive Shell</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>In the Spark shell, the SparkContext is created when the shell
launches. The SparkContext is held in the variable sc . The master
PySpark
|
43for the interactive shell can be set by using the --master argument
when the shell is launched. To start an interactive shell, run the
pyspark command:
$ pyspark --master local[4]
...
Welcome to</p>
<hr />
<p><strong>
/ </strong>/<strong> <strong><em> </em></strong></strong>/ /<strong>
_\ \/ _ \/ _ `/ </strong>/ '<em>/
/<strong> / .</strong>/_,</em>/<em>/ /</em>/_\
version 1.5.0
/_/
Using Python version 2.7.10 (default, Jul 13 2015 12:05:58)
SparkContext available as sc, HiveContext available as sqlCon-
text.</p>
<blockquote>
<blockquote>
<blockquote>
<p>For a complete list of options, run pyspark --help .</p>
</blockquote>
</blockquote>
</blockquote></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Self-Contained Applications</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Self-contained applications must first create a SparkContext object
before using any Spark methods. The master can be set when the
SparkContext() method is called:
sc = SparkContext(master='local[4]')
To execute self-contained applications, they must be submitted to
the spark-submit script. The spark-submit script contains many
options; to see a complete listing, run spark-submit --help from
the command line:
$ spark-submit --master local spark_app.py</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Resilient Distributed Datasets (RDDs)</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Resilient Distributed Datasets (RDDs) are the fundamental pro‐
gramming abstraction in Spark. RDDs are immutable collections of
data, partitioned across machines, that enable operations to be per‐
formed on elements in parallel. RDDs can be constructed in multi‐
ple ways: by parallelizing existing Python collections, by referencing
files in an external storage system such as HDFS, or by applying
transformations to existing RDDs.</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Creating RDDs from Collections</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>RDDs can be created from a Python collection by calling the Spark
Context.parallelize() method. The elements of the collection are
copied to form a distributed dataset that can be operated on in par‐
allel. The following example creates a parallelized collection from a
Python list:</p>
<blockquote>
<blockquote>
<blockquote>
<p>data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)
rdd.glom().collect()
...
[[1, 2, 3, 4, 5]]
The RDD.glom() method returns a list of all of the elements within
each partition, and the RDD.collect() method brings all the ele‐
ments to the driver node. The result, [[1, 2, 3, 4, 5]] , is the
original collection within a list.
To specify the number of partitions an RDD should be created with,
a second argument can be passed to the parallelize() method.
The following example creates an RDD from the same Python col‐
lection in the previous example, except this time four partitions are
created:
rdd = sc.parallelize(data, 4)
rdd.glom().collect()
...
[[1], [2], [3], [4, 5]]
Using the glom() and collect() methods, the RDD created in this
example contains four inner lists: [1] , [2] , [3] , and [4, 5] . The
number of inner lists represents the number of partitions within the
RDD.</p>
</blockquote>
</blockquote>
</blockquote></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Creating RDDs from External Sources</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>RDDs can also be created from files using the SparkContext.text
File() method. Spark can read files residing on the local filesystem,
any storage source supported by Hadoop, Amazon S3, and so on.
Spark supports text files, SequenceFiles, any other Hadoop Input‐
Format, directories, compressed files, and wildcards, e.g., my/direc‐
tory/*.txt. The following example creates a distributed dataset from a
file located on the local filesystem:
Resilient Distributed Datasets (RDDs)
|
45&gt;&gt;&gt; distFile = sc.textFile('data.txt')</p>
<blockquote>
<blockquote>
<blockquote>
<p>distFile.glom().collect()
...
[[u'jack be nimble', u'jack be quick', u'jack jumped over the
candlestick']]
As before, the glom() and collect() methods allow the RDD to be
displayed in its partitions. This result shows that distFile only has
a single partition.
Similar to the parallelize() method, the textFile() method
takes a second parameter that specifies the number of partitions to
create. The following example creates an RDD with three partitions
from the input file:
distFile = sc.textFile('data.txt', 3)
distFile.glom().collect()
...
[[u'jack be nimble', u'jack be quick'], [u'jack jumped over
the candlestick'], []]</p>
</blockquote>
</blockquote>
</blockquote></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>RDD Operations</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>RDDs support two types of operations: transformations and actions.
Transformations create new datasets from existing ones, and actions
run a computation on the dataset and return results to the driver
program.
Transformations are lazy: that is, their results are not computed
immediately. Instead, Spark remembers all of the transformations
applied to a base dataset. Transformations are computed when an
action requires a result to be returned to the driver program. This
allows Spark to operate efficiently and only transfer the results of the
transformations before an action.
By default, transformations may be recomputed each time an action
is performed on it. This allows Spark to efficiently utilize memory,
but it may utilize more processing resources if the same transforma‐
tions are constantly being processed. To ensure a transformation is
only computed once, the resulting RDD can be persisted in memory
using the RDD.cache() method.</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>RDD Workflow</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>The general workflow for working with RDDs is as follows:
1. Create an RDD from a data source.
2. Apply transformations to an RDD.
3. Apply actions to an RDD.
The following example uses this workflow to calculate the number
of characters in a file:</p>
<blockquote>
<blockquote>
<blockquote>
<p>59
lines = sc.textFile('data.txt')
line_lengths = lines.map(lambda x: len(x))
document_length = line_lengths.reduce(lambda x,y: x+y)
print document_length
The first statement creates an RDD from the external file data.txt.
This file is not loaded at this point; the variable lines is just a
pointer to the external source. The second statement performs a
transformation on the base RDD by using the map() function to cal‐
culate the number of characters in each line. The variable
line_lengths is not immediately computed due to the laziness of
transformations. Finally, the reduce() method is called, which is an
action. At this point, Spark divides the computations into tasks to
run on separate machines. Each machine runs both the map and
reduction on its local data, returning only the results to the driver
program.
If the application were to use line_lengths again, it would be best
to persist the result of the map transformation to ensure that the
map would not be recomputed. The following line will save
line_lengths into memory after the first time it is computed:
line_lengths.persist()</p>
</blockquote>
</blockquote>
</blockquote></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Transformations</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Transformations create new datasets from existing ones. Lazy evalu‐
ation of transformation allows Spark to remember the set of trans‐
formations applied to the base RDD. This enables Spark to optimize
the required calculations.
This section describes some of Spark’s most common transforma‐
tions. For a full listing of transformations, refer to Spark’s Python
RDD API doc.
map. The map(func) function returns a new RDD by applying a
function, func, to each element of the source. The following example
multiplies each element of the source RDD by two:</p>
<blockquote>
<blockquote>
<blockquote>
<p>[2,
data = [1, 2, 3, 4, 5, 6]
rdd = sc.parallelize(data)
map_result = rdd.map(lambda x: x * 2)
map_result.collect()
4, 6, 8, 10, 12]
filter. The filter(func) function returns a new RDD containing
only the elements of the source that the supplied function returns as
true. The following example returns only the even numbers from
the source RDD:</p>
<p>[2,
48
|
data = [1, 2, 3, 4, 5, 6]
filter_result = rdd.filter(lambda x: x % 2 == 0)
filter_result.collect()
4, 6]</p>
</blockquote>
</blockquote>
</blockquote>
<p>distinct. The distinct() method returns a new RDD containing
only the distinct elements from the source RDD. The following
example returns the unique elements in a list:</p>
<blockquote>
<blockquote>
<blockquote>
<p>[4,
data = [1, 2, 3, 2, 4, 1]
rdd = sc.parallelize(data)
distinct_result = rdd.distinct()
distinct_result.collect()
1, 2, 3]</p>
</blockquote>
</blockquote>
</blockquote>
<p>flatMap. The flatMap(func) function is similar to the map() func‐
tion, except it returns a flattened version of the results. For compari‐
son, the following examples return the original element from the
source RDD and its square. The example using the map() function
returns the pairs as a list within a list:</p>
<blockquote>
<blockquote>
<blockquote>
<p>data = [1, 2, 3, 4]
rdd = sc.parallelize(data)
map = rdd.map(lambda x: [x, pow(x,2)])
map.collect()
[[1, 1], [2, 4], [3, 9], [4, 16]]</p>
</blockquote>
</blockquote>
</blockquote>
<p>While the flatMap() function concatenates the results, returning a
single list:</p>
<blockquote>
<blockquote>
<blockquote>
<p>[1,
rdd = sc.parallelize()
flat_map = rdd.flatMap(lambda x: [x, pow(x,2)])
flat_map.collect()
1, 2, 4, 3, 9, 4, 16]</p>
</blockquote>
</blockquote>
</blockquote></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Actions</h2>
      <h3></h3>
    </hgroup>
    <article >
      <p>Actions cause Spark to compute transformations. After transforms
are computed on the cluster, the result is returned to the driver pro‐
gram.
The following section describes some of Spark’s most common
actions. For a full listing of actions, refer to Spark’s Python RDD API
doc.
reduce. The reduce() method aggregates elements in an RDD
using a function, which takes two arguments and returns one. The
function used in the reduce method is commutative and associative,
ensuring that it can be correctly computed in parallel. The following
example returns the product of all of the elements in the RDD:</p>
<blockquote>
<blockquote>
<blockquote>
<p>data = [1, 2, 3]
rdd = sc.parallelize(data)
Resilient Distributed Datasets (RDDs)
|
49&gt;&gt;&gt; rdd.reduce(lambda a, b: a * b)
6
take. The take(n) method returns an array with the first n ele‐
ments of the RDD. The following example returns the first two ele‐
ments of an RDD:</p>
<p>[1,
data = [1, 2, 3]
rdd = sc.parallelize(data)
rdd.take(2)
2]
collect. The collect() method returns all of the elements of the
RDD as an array. The following example returns all of the elements
from an RDD:</p>
<p>[1,
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)
rdd.collect()
2, 3, 4, 5]
It is important to note that calling collect() on large datasets could
cause the driver to run out of memory. To inspect large RDDs, the
take() and collect() methods can be used to inspect the top n ele‐
ments of a large RDD. The following example will return the first
100 elements of the RDD to the driver:
rdd.take(100).collect()
takeOrdered. The takeOrdered(n, key=func) method returns the
first n elements of the RDD, in their natural order, or as specified by
the function func. The following example returns the first four ele‐
ments of the RDD in descending order:</p>
<p>[6,
data = [6,1,5,2,4,3]
rdd = sc.parallelize(data)
rdd.takeOrdered(4, lambda s: -s)
5, 4, 3]</p>
</blockquote>
</blockquote>
</blockquote></article>
 
</slide>


<slide class="thank-you-slide segue nobackground">
  <aside class="gdbar right"><img src="theme/logo.png"></aside>
  <article class="flexbox vleft auto-fadein">
    <h2>&lt;Domande?&gt;</h2>
  </article>
  <p class="auto-fadein" data-config-contact>
    Michele Tomaiuolo
    <br>
    Palazzina 1, int. 5708
    <br>
    Ingegneria dell'Informazione, UniPR
    <br>
    <a href="http://sowide.unipr.it/tomamic">sowide.unipr.it/tomamic</a>
  </p>
</slide>

<slide class="backdrop"></slide>

</slides>

</body>
</html>